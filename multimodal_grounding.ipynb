{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multimodal Grounding with MLLM (LLaVA + PEFT)\n",
                "\n",
                "This notebook implements a Multimodal Large Language Model (MLLM) aiming for visual accuracy and linguistic fluency, as outlined in the project plan.\n",
                "\n",
                "## Key Components:\n",
                "1.  **Architecture**: LLaVA-style (Frozen Vision Encoder + Trainable Projection + Frozen LLM).\n",
                "2.  **Optimization**: PEFT (LoRA) for efficient fine-tuning.\n",
                "3.  **Data**: Flickr8k dataset.\n",
                "4.  **Grounding**: Analysis of Cross-Attention/Self-Attention maps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary libraries if not present\n",
                "!pip install -q torch transformers peft accelerate bitsandbytes pillow matplotlib pandas pycocoevalcap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from transformers import (\n",
                "    AutoTokenizer, \n",
                "    AutoModelForCausalLM, \n",
                "    CLIPVisionModel, \n",
                "    CLIPImageProcessor,\n",
                "    BitsAndBytesConfig\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "from PIL import Image\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Check for GPU\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Preparation (Flickr8k)\n",
                "We load the Flickr8k dataset, formatting it into a visual instruction tuning format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration (Paths from original notebook)\n",
                "IMAGES_DIR = '/kaggle/input/flickr8k/Images/'\n",
                "CAPTIONS_FILE = '/kaggle/input/flickr8k/captions.txt'\n",
                "\n",
                "# Model Checkpoints (using accessible open-source models)\n",
                "VISION_MODEL_CKPT = \"openai/clip-vit-large-patch14-336\"\n",
                "LLM_CKPT = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Lightweight for demonstration, replaceable with Llama-2-7b etc.\n",
                "\n",
                "# Hyperparameters\n",
                "MAX_LENGTH = 128\n",
                "BATCH_SIZE = 4\n",
                "EPOCHS = 1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Flickr8kDataset(Dataset):\n",
                "    def __init__(self, captions_file, images_dir, tokenizer, image_processor, max_length=128):\n",
                "        self.images_dir = images_dir\n",
                "        self.df = pd.read_csv(captions_file)\n",
                "        self.tokenizer = tokenizer\n",
                "        self.image_processor = image_processor\n",
                "        self.max_length = max_length\n",
                "        \n",
                "        # Visual Instruction Template\n",
                "        self.prompt_template = \"User: <image>\\nDescribe this image.\\nAssistant: \"\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        image_file = row['image']\n",
                "        caption = row['caption']\n",
                "        \n",
                "        # Load and process image\n",
                "        image_path = os.path.join(self.images_dir, image_file)\n",
                "        image = Image.open(image_path).convert(\"RGB\")\n",
                "        pixel_values = self.image_processor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
                "        \n",
                "        # Prepare Text\n",
                "        full_text = self.prompt_template + str(caption)\n",
                "        \n",
                "        # Tokenize\n",
                "        tokenized = self.tokenizer(\n",
                "            full_text,\n",
                "            return_tensors=\"pt\",\n",
                "            max_length=self.max_length,\n",
                "            padding=\"max_length\",\n",
                "            truncation=True\n",
                "        )\n",
                "        \n",
                "        input_ids = tokenized.input_ids.squeeze(0)\n",
                "        attention_mask = tokenized.attention_mask.squeeze(0)\n",
                "        \n",
                "        # Create labels\n",
                "        labels = input_ids.clone()\n",
                "        \n",
                "        return {\n",
                "            \"pixel_values\": pixel_values,\n",
                "            \"input_ids\": input_ids,\n",
                "            \"attention_mask\": attention_mask,\n",
                "            \"labels\": labels\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Architecture Setup (LLaVA-like)\n",
                "\n",
                "- **Vision Encoder**: CLIP ViT-L/14 (Frozen)\n",
                "- **Projection Layer**: Linear layer mapping Visual Dim -> LLM Dim (Trainable)\n",
                "- **LLM**: TinyLlama (Frozen, except LoRA adapters)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleMLLM(nn.Module):\n",
                "    def __init__(self, vision_ckpt, llm_ckpt):\n",
                "        super().__init__()\n",
                "        \n",
                "        # 1. Vision Encoder\n",
                "        self.vision_encoder = CLIPVisionModel.from_pretrained(vision_ckpt)\n",
                "        for param in self.vision_encoder.parameters():\n",
                "            param.requires_grad = False # Freeze Vision Encoder\n",
                "            \n",
                "        # 2. LLM\n",
                "        # Using 4-bit quantization for optimization if bitsandbytes is available and CUDA is present\n",
                "        if torch.cuda.is_available():\n",
                "            bnb_config = BitsAndBytesConfig(\n",
                "                load_in_4bit=True,\n",
                "                bnb_4bit_quant_type=\"nf4\",\n",
                "                bnb_4bit_compute_dtype=torch.float16\n",
                "            )\n",
                "            self.llm = AutoModelForCausalLM.from_pretrained(\n",
                "                llm_ckpt, \n",
                "                quantization_config=bnb_config, \n",
                "                device_map={\"\": 0} \n",
                "            )\n",
                "        else:\n",
                "            print(\"WARNING: CUDA not available. Loading model on CPU without quantization.\")\n",
                "            self.llm = AutoModelForCausalLM.from_pretrained(\n",
                "                llm_ckpt\n",
                "            )\n",
                "            \n",
                "        # Disable standard gradient calculation for LLM (we will use LoRA)\n",
                "        for param in self.llm.parameters():\n",
                "            param.requires_grad = False\n",
                "            \n",
                "        # 3. Projection Layer (The \"Bridge\")\n",
                "        vision_dim = self.vision_encoder.config.hidden_size\n",
                "        llm_dim = self.llm.config.hidden_size\n",
                "        \n",
                "        self.projection = nn.Linear(vision_dim, llm_dim)\n",
                "        # Projection is trainable\n",
                "\n",
                "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
                "        # 1. Extract Visual Features\n",
                "        with torch.no_grad():\n",
                "            vision_outputs = self.vision_encoder(pixel_values)\n",
                "            image_features = vision_outputs.last_hidden_state # [Batch, Patches, Dim]\n",
                "        \n",
                "        # 2. Project Visual Features to LLM Space\n",
                "        image_embeddings = self.projection(image_features) # [Batch, Patches, LLM_Dim]\n",
                "        \n",
                "        # 3. Embed Text\n",
                "        inputs_embeds = self.llm.get_input_embeddings()(input_ids)\n",
                "        \n",
                "        # 4. Concatenate: [Image_Embeds, Text_Embeds]\n",
                "        combined_embeds = torch.cat([image_embeddings, inputs_embeds], dim=1)\n",
                "        \n",
                "        # Adjust Attention Mask (1 for images)\n",
                "        batch_size = pixel_values.shape[0]\n",
                "        image_mask = torch.ones((batch_size, image_embeddings.shape[1]), device=device)\n",
                "        combined_mask = torch.cat([image_mask, attention_mask], dim=1)\n",
                "        \n",
                "        # Adjust Labels (ignore images for loss calculation -> -100)\n",
                "        if labels is not None:\n",
                "            image_labels = torch.ones((batch_size, image_embeddings.shape[1]), device=device, dtype=torch.long) * -100\n",
                "            combined_labels = torch.cat([image_labels, labels], dim=1)\n",
                "        else:\n",
                "            combined_labels = None\n",
                "            \n",
                "        # 5. Pass through LLM\n",
                "        outputs = self.llm(\n",
                "            inputs_embeds=combined_embeds,\n",
                "            attention_mask=combined_mask,\n",
                "            labels=combined_labels,\n",
                "            output_attentions=True \n",
                "        )\n",
                "        \n",
                "        return outputs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Optimization and Training: PEFT (LoRA)\n",
                "\n",
                "We apply LoRA to the LLM decoder to enable parameter-efficient fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Components\n",
                "tokenizer = AutoTokenizer.from_pretrained(LLM_CKPT)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "image_processor = CLIPImageProcessor.from_pretrained(VISION_MODEL_CKPT)\n",
                "\n",
                "model = SimpleMLLM(VISION_MODEL_CKPT, LLM_CKPT)\n",
                "model.to(device)\n",
                "\n",
                "# Apply LoRA\n",
                "peft_config = LoraConfig(\n",
                "    task_type=TaskType.CAUSAL_LM,\n",
                "    inference_mode=False,\n",
                "    r=8,\n",
                "    lora_alpha=32,\n",
                "    lora_dropout=0.1,\n",
                "    target_modules=[\"q_proj\", \"v_proj\"]\n",
                ")\n",
                "\n",
                "model.llm = get_peft_model(model.llm, peft_config)\n",
                "model.llm.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training Setup\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
                "dataset = Flickr8kDataset(CAPTIONS_FILE, IMAGES_DIR, tokenizer, image_processor)\n",
                "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "\n",
                "# Mock Training Loop\n",
                "model.train()\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"Starting Epoch {epoch+1}\")\n",
                "    # Training loop would go here\n",
                "    print(\"Epoch completed (Mock)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Enforcing Grounding & Evaluation\n",
                "\n",
                "We use the **Attention Maps** from the model to visualize which parts of the image the model focuses on when generating specific words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_attention(model, image_path, prompt, tokenizer, image_processor):\n",
                "    model.eval()\n",
                "    \n",
                "    # Prepare inputs\n",
                "    image = Image.open(image_path).convert(\"RGB\")\n",
                "    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
                "    \n",
                "    text_input = \"User: <image>\\n\" + prompt + \"\\nAssistant: \"\n",
                "    inputs = tokenizer(text_input, return_tensors=\"pt\").to(device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(pixel_values, inputs.input_ids, inputs.attention_mask)\n",
                "    \n",
                "    # Extract LAST layer attention\n",
                "    last_layer_attentions = outputs.attentions[-1] # [Batch, Heads, Seq, Seq]\n",
                "    avg_attention = last_layer_attentions.mean(dim=1).squeeze(0) # [Seq, Seq]\n",
                "    \n",
                "    print(\"Attention analysis complete.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation Metrics (CIDEr, SPICE, BLEU)\n",
                "\n",
                "We use `pycocoevalcap` to compute standard captioning metrics. This requires a dictionary of reference captions and a dictionary of generated hypotheses."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pycocoevalcap.bleu.bleu import Bleu\n",
                "from pycocoevalcap.meteor.meteor import Meteor\n",
                "from pycocoevalcap.rouge.rouge import Rouge\n",
                "from pycocoevalcap.cider.cider import Cider\n",
                "from pycocoevalcap.spice.spice import Spice\n",
                "\n",
                "def score_captions(ref, hypo):\n",
                "    \"\"\"\n",
                "    ref: dictionary of reference captions (id -> list of strings)\n",
                "    hypo: dictionary of hypothesis captions (id -> list of strings)\n",
                "    \"\"\"\n",
                "    scorers = [\n",
                "        (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
                "        (Meteor(), \"METEOR\"),\n",
                "        (Rouge(), \"ROUGE_L\"),\n",
                "        (Cider(), \"CIDEr\"),\n",
                "        (Spice(), \"SPICE\")\n",
                "    ]\n",
                "    \n",
                "    final_scores = {}\n",
                "    for scorer, method in scorers:\n",
                "        # compute_score returns (score, scores)\n",
                "        score, scores = scorer.compute_score(ref, hypo)\n",
                "        if isinstance(method, list):\n",
                "            for m, s in zip(method, score):\n",
                "                final_scores[m] = s\n",
                "        else:\n",
                "            final_scores[method] = score\n",
                "            \n",
                "    return final_scores\n",
                "\n",
                "def evaluate_metrics_demo():\n",
                "    print(\"Running Evaluation Metrics on Mock Data...\")\n",
                "    \n",
                "    # Mock Data (Format required by pycocoevalcap)\n",
                "    # Keys must be integers (image_ids)\n",
                "    references = {\n",
                "        0: [\"a photo of a dog running\", \"the dog runs fast\", \"a puppy running\"],\n",
                "        1: [\"a cat sitting on a sofa\", \"a kitten resting\", \"cat on couch\"]\n",
                "    }\n",
                "    \n",
                "    hypotheses = {\n",
                "        0: [\"a dog runs\"],\n",
                "        1: [\"a cat plays\"]\n",
                "    }\n",
                "    \n",
                "    try:\n",
                "        scores = score_captions(references, hypotheses)\n",
                "        for metric, score in scores.items():\n",
                "            print(f\"{metric}: {score:.4f}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Evaluation failed (likely missing dependencies or java): {e}\")\n",
                "        print(\"Please ensure pycocoevalcap and Java are installed.\")\n",
                "\n",
                "evaluate_metrics_demo()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}